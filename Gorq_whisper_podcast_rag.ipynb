{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMihMXGk1fRz41kq0cIFMaG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahupra1357/LLMAgents/blob/main/Gorq_whisper_podcast_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rB4FlNPxlywR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "groq_api_key = os.getenv('GROQ_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq  -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaKxCXn0oZks",
        "outputId": "96fc8771-3f4a-4f78-8286-182db7e8fde6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m122.9/125.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#from pydub import AudioSegment"
      ],
      "metadata": {
        "id": "hN-EESLDoZhs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Groq(api_key = groq_api_key)\n",
        "model = 'whisper-large-v3'"
      ],
      "metadata": {
        "id": "Hga_qa0PoZcG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"Groq's AI Chip Breaks Speed Records.mp3\"\n",
        "def audio_to_text(filepath):\n",
        "    with open(filepath, \"rb\") as file:\n",
        "        translation = client.audio.translations.create(\n",
        "            file=(filepath, file.read()),\n",
        "            model=\"whisper-large-v3\",\n",
        "        )\n",
        "    return translation.text\n",
        "\n",
        "translation_text = audio_to_text(filepath)\n",
        "\n",
        "# Show just the beginning of the transcription\n",
        "print(translation_text[:2000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooZ0iskxoZY1",
        "outputId": "e11e8f9f-dac6-41c9-d839-a7580c40682d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Welcome back, you're with Connect the World. I'm Becky Anderson. We're at the World Government Summit in Dubai and one thing I've noticed here is that whenever a discussion about artificial intelligence takes place, the rooms here, the huge halls get packed. That is because some of the leading minds behind the technological revolution have been gathered here at the conference over the past couple of days and in AI's race to the top my next guest is sprinting at speeds never seen before Jonathan Ross is the brain behind GROK the world's first language processing unit now before I lose you in the technological jargon of AI let me put it this way what Ross created is a chip that can run programs like Meta's Llama 2 model for example faster than anything else in the world. 10 to 100 times faster in fact and he's here with me now to explain how that is possible. Before I ask you that, grok, why grok? Thank you Becky. It's grok and we spell it with a Q and it's because it comes from a science fiction novel and it means to understand something deeply and with empathy. Of course it Tell us about your chip and what makes Brock Chip LPU different from other AI chips and accelerators. I have to tell our viewers that the NVIDIA CEO was here, of course, this week at the beginning of the week. So we've had all the greatest minds in here. What's your story? Well, asking me how the chip works before I show you what it does is a bit like asking how a magic trick works before showing you the magic trick. You're going to get bored, but I'll give it a shot. Cool. So most chips they don have enough memory inside of them Sort of like if you were building cars and you use a giant factory and you need about a million square feet of assembly line space Well if you don have a building that large enough to fit that then you need to set up part of the assembly line tear it down over and over again Right And that slow and it takes a lot of time And that what happens with the GPU You have to re\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transcript_chat_completion(client, transcript, user_question):\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": '''Use this transcript or transcripts to answer any user questions, citing specific quotes:\n",
        "\n",
        "                {transcript}\n",
        "                '''.format(transcript=transcript)\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_question,\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama3-8b-8192\",\n",
        "    )\n",
        "\n",
        "    print(chat_completion.choices[0].message.content)\n",
        "\n",
        "user_question = \"Explain the importance of fast language models\"\n",
        "transcript_chat_completion(client, translation_text, user_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX_N0Icro8cV",
        "outputId": "6182123f-38e1-44e2-fa03-ca9117c9b85c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to Jonathan Ross, the creator of GROK, the world's first language processing unit, speed is crucial in language models because it directly affects user engagement. He mentioned that if a website takes an extra 100 milliseconds to load, it will lead to an 8% decrease in user engagement on desktop and a 34% decrease on mobile. This is because users expect fast responses and have little patience.\n",
            "\n",
            "Faster language models, like GROK, can process and generate human language in a natural way, making interactions feel more like conversations with a human. This is because they can process and respond to user input quickly, without any delays or lags. This natural experience can lead to increased user engagement, satisfaction, and ultimately, loyalty.\n",
            "\n",
            "Ross also explained that speed is a differentiator for GROK, making it stand out from other large language models. He emphasized that GROK doesn't create new language models but instead accelerates existing ones, making them faster and more natural. This can significantly impact various industries and applications, such as:\n",
            "\n",
            "1. Customer service: Faster language models can enable more efficient and effective customer interactions, improving overall customer experience and satisfaction.\n",
            "2. Virtual assistants: Faster models can enable more natural and conversational interactions with virtual assistants, making them more practical and user-friendly.\n",
            "3. Language translation: Faster language models can facilitate more accurate and efficient language translation, enabling real-time communication across languages.\n",
            "4. Chatbots: Faster models can improve the overall conversational experience with chatbots, making them more engaging and useful for users.\n",
            "\n",
            "In summary, fast language models are important because they can lead to increased user engagement, satisfaction, and loyalty. They can also differentiate applications and services, enabling more natural and practical interactions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GMRwVt-uo8Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r58jjrcJo8Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_v21jTPwo8T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OCWUcF1Zo8Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "88X5lNWzo8Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JDMu8BEIo8KW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}